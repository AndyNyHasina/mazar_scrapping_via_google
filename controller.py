import os
from langchain_openai import  ChatOpenAI

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser



from dotenv import load_dotenv

current_dir = os.path.dirname(os.path.abspath(__file__))
dotenv_path = os.path.join(current_dir, '.env')
load_dotenv(dotenv_path)

class GPT_ask:
    PROMPTS = {

        
        
"assistant": """
Tu es un assistant expert en extraction et structuration d‚Äôinformations √† partir de profils LinkedIn ou de CV au format texte brut.
Tu dois analyser minutieusement le texte re√ßu pour extraire, √©tape par √©tape, toutes les informations pertinentes du profil, m√™me si elles ne sont pas parfaitement organis√©es.

Proc√®de de la fa√ßon suivante‚ÄØ:

√âtape 1‚ÄØ: Identifie et extrait les informations personnelles (nom complet, poste actuel ou principal, localisation, coordonn√©es √©ventuelles).

√âtape 2‚ÄØ: Recherche les indications sur le r√©seau professionnel (nombre d‚Äôabonn√©s, nombre de relations, etc.) et ajoute-les si pr√©sents.

√âtape 3‚ÄØ: Parcours l‚Äôint√©gralit√© du texte pour retrouver la section Exp√©rience professionnelle. Pour chaque exp√©rience list√©e, extrait‚ÄØ:

intitul√© du poste

entreprise

p√©riode (d√©but, fin ou ¬´‚ÄØactuel‚ÄØ¬ª)

dur√©e

lieu

description (missions, responsabilit√©s, r√©alisations, etc.)

√âtape 4‚ÄØ: Recherche la section Formation. Pour chaque dipl√¥me ou formation, extrait‚ÄØ:

intitul√© ou nom du dipl√¥me

√©tablissement

dates (d√©but et fin)

d√©tails √©ventuels (activit√©s, sp√©cialisations, mentions)

√âtape 5‚ÄØ: Recherche la section Langues et extrait pour chaque langue‚ÄØ:

nom de la langue

niveau de ma√Ætrise

√âtape 6‚ÄØ: Si pr√©sentes, identifie d‚Äôautres rubriques importantes (Comp√©tences, Centres d‚Äôint√©r√™t, etc.) et regroupe-les dans une section ¬´‚ÄØautres‚ÄØ¬ª.

IMPORTANT‚ÄØ:

IGNORE toute la section ou tout contenu li√© √†‚ÄØ:

üì¢ Activit√©

Publications et posts r√©cents

Mentions "Aim√© par Christophe Mazars"

Contenu partag√© (posts LinkedIn, posts aim√©s, publications partag√©es, etc.)

N‚Äôinvente aucune information. Si une information ou une section n‚Äôest pas trouv√©e dans le texte d‚Äôentr√©e, retourne strictement la valeur null (et pas de texte, ni de valeur par d√©faut, ni d‚Äôinterpr√©tation).

Retourne UNIQUEMENT un objet JSON structur√©, sans aucune explication, suivant ce mod√®le‚ÄØ:

{{{{ 
  "informations_personnelles": {{
    "nom": null,
    "titre": null,
    "localisation": null,
    "coordonnees": null
  }},
  "reseau": {{
    "abonnes": null,
    "relations": null
  }},
  "experiences": [
    {{
      "poste": null,
      "entreprise": null,
      "date_debut": null,
      "date_fin": null,
      "duree": null,
      "lieu": null,
      "description": null
    }}
  ],
  "formations": [
    {{
      "diplome": null,
      "etablissement": null,
      "date_debut": null,
      "date_fin": null,
      "details": null
    }}
  ],
  "langues": [
    {{
      "langue": null,
      "niveau": null
    }}
  ],
  "autres": null
}}}}

Respecte absolument ce sch√©ma. N‚Äôinvente rien, ne comble aucun champ. Si une donn√©e n‚Äôest pas explicitement trouv√©e, mets strictement null.

Structure pr√©cis√©ment les informations pour faciliter leur int√©gration en base de donn√©es.
"""
    }
    
    
    def __init__(self, prompt_type: str = "assistant", custom_prompt: str = None):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("‚ùå OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
        
        # Choisir le prompt
        if custom_prompt:
            self.system_prompt = custom_prompt
        elif prompt_type in self.PROMPTS:
            self.system_prompt = self.PROMPTS[prompt_type]
        else:
            self.system_prompt = self.PROMPTS["assistant"]
        
        # Configuration LLM
        self.llm = ChatOpenAI(
            temperature=0.8,
            openai_api_key=self.openai_api_key,
            model="gpt-4-1106-preview"  # GPT-4.1 officiel OpenAI (juillet 2025)
        )


        
        self.prompt_type = prompt_type

        self._setup_chain()
    
    def _setup_chain(self):
        """Configure la cha√Æne LangChain moderne"""
        system_message = SystemMessagePromptTemplate.from_template(self.system_prompt)
        human_message = HumanMessagePromptTemplate.from_template("{input}")
        self.prompt_template = ChatPromptTemplate.from_messages([system_message, human_message])
        self.output_parser = StrOutputParser()
        self.chain = self.prompt_template | self.llm | self.output_parser
    
    def conversation(self, user_input: str ) -> str:
        """G√©n√®re une r√©ponse selon le mode"""
        try:
            result = self.chain.invoke({"input": user_input})
            print("***********")
            return result.strip() if isinstance(result, str) else str(result).strip()
        except Exception as e:
            return f"‚ùå Erreur lors de la g√©n√©ration : {str(e)}\nüí° V√©rifiez votre cl√© API OpenAI"
    

    
    def change_role(self, prompt_type: str):
        """Change le r√¥le/prompt en cours"""
        if prompt_type in self.PROMPTS:
            self.prompt_type = prompt_type
            self.system_prompt = self.PROMPTS[prompt_type]
            self._setup_chain()
            print(f"‚úÖ R√¥le chang√© vers : {prompt_type}")
        else:
            print(f"‚ùå R√¥le '{prompt_type}' non reconnu. R√¥les disponibles : {list(self.PROMPTS.keys())}")
    
    def reset_conversation(self):
        """Reset la configuration"""
        print("‚úÖ Configuration r√©initialis√©e")
        self._setup_chain()
        


